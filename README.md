# Relation-Extraxtion

Finally, we'll show how to use the transformers library to load a pre-trained transformer model, specifically the BERT model, and use it to provide the embeddings for text. These embeddings can be fed into any model to predict sentiment, however we use a gated recurrent unit (GRU) and Long short-term memory (LSTM).

**If you find any mistakes or disagree with any of the explanations, please do not hesitate to [submit an issue](https://github.com/AidinZe/Relation-Extraxtion-/issues/new). I welcome any feedback, positive or negative!**
